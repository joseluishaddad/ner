----------------------------------------------------------------------------------------------Model 1 (DELETED)
- Name: "daphne_entities"
- TRAIN_DATA size: <=500
- n_iter: 30
- base_model: NONE
- drop: 0.35

Observations: 
- gradient (or equivalent) seems to be stucked around a point between 800 and 1000
- after a while it keeps improving

Logs:
Created blank 'en' model
Losses {'ner': 4489.412552776306}
Losses {'ner': 1948.97124134601}
Losses {'ner': 1815.4649216474106}
Losses {'ner': 1693.0347360453961}
Losses {'ner': 1544.0488733216985}
Losses {'ner': 976.6177784424284}
Losses {'ner': 1234.0980577643409}
Losses {'ner': 1131.9077526832014}
Losses {'ner': 882.8515712833264}
Losses {'ner': 953.8377552783967}
Losses {'ner': 1196.5752645508107}
Losses {'ner': 987.3936979715917}
Losses {'ner': 1132.165592013147}
Losses {'ner': 804.8115571740011}
Losses {'ner': 1023.8783625776966}
Losses {'ner': 1039.5677662286419}
Losses {'ner': 1076.8680460703497}
Losses {'ner': 1026.608085923413}
Losses {'ner': 755.0296950525235}
Losses {'ner': 951.9372848702976}
Losses {'ner': 654.9285097209967}
Losses {'ner': 855.378645022301}
Losses {'ner': 678.7416938603727}
Losses {'ner': 591.6910209382713}
Losses {'ner': 560.5515034472505}
Losses {'ner': 958.397553736115}
Losses {'ner': 649.1730262443768}
Losses {'ner': 738.2431910281547}
Losses {'ner': 655.9258121367895}
Losses {'ner': 444.32084935073686}


------------------------------------------------------------------------------------------------------Model 2
- Name: "daphne_entities"
- TRAIN_DATA size: <=500
- n_iter: 20
- base_model: "en_core_web_sm"
- drop: 0.35

Observations: 
- started way worse than the previous experiment (10 times worse)
- slowly but consistently decreasing losses
- it is not just improving now

Logs:
Loaded model 'en_core_web_sm'
Losses {'ner': 47582.825801131235}
Losses {'ner': 45801.719451731544}
Losses {'ner': 45468.17972561895}
Losses {'ner': 45246.29488963052}
Losses {'ner': 44933.70249993259}
Losses {'ner': 44769.69267426797}
Losses {'ner': 44376.268556476636}
Losses {'ner': 44206.68637771745}
Losses {'ner': 44174.400953653494}
Losses {'ner': 44074.99817721757}
Losses {'ner': 44087.87211754389}
Losses {'ner': 43904.03830339907}
Losses {'ner': 43359.24981199539}
Losses {'ner': 43555.5708537874}
Losses {'ner': 44004.42196800396}
Losses {'ner': 43719.137884859054}
Losses {'ner': 43531.03373148747}
Losses {'ner': 43435.11209259271}
Losses {'ner': 43169.161581589135}
Losses {'ner': 42699.35215634399}

------------------------------------------------------------------------------------------------------Model 3
- Name: "daphne_entities_2"
- TRAIN_DATA size: <=500
- n_iter: 30
- base_model: NONE
- drop: 0.2

Observations: 
- started better than model 2 and by the 5th iteration it is bellow a thousand 
- improved, got worse, then improved again
- bouncing between 600 and 900
- made it to 390 after a while

Logs:
(ACCIDENTALLY ERASED THEM)

----------------------------------------------------------------------------------------------------Model 4
- Name: "daphne_entities_2" (DRIVE) (MODEL NOT SAVED)
- TRAIN_DATA size: <=200
- n_iter: 30
- base_model: NONE
- drop: 0.35

Observations: 
- started better and keeps improving by the 5th iteration
- bouncing around 400, but made it to 340 at some point


Logs:
Created blank 'en' model
Losses {'ner': 2073.269552979174}
Losses {'ner': 1388.6238628475169}
Losses {'ner': 1307.819842815718}
Losses {'ner': 921.3476050709899}
Losses {'ner': 758.097082534308}
Losses {'ner': 717.8943865045678}
Losses {'ner': 888.1052495905125}
Losses {'ner': 836.3778616191663}
Losses {'ner': 754.2681494460693}
Losses {'ner': 679.9740406204759}
Losses {'ner': 669.2573379379787}
Losses {'ner': 562.3351459588935}
Losses {'ner': 480.05526026612233}
Losses {'ner': 340.99273439206445}
Losses {'ner': 361.88883826820233}
Losses {'ner': 543.7084096632884}
Losses {'ner': 692.7137613983845}
Losses {'ner': 678.9115231610708}
Losses {'ner': 644.3140324419988}
Losses {'ner': 638.230722959596}
Losses {'ner': 458.0539907161155}
Losses {'ner': 555.747720640369}
Losses {'ner': 563.8842628958106}
Losses {'ner': 682.5719067029851}
Losses {'ner': 654.2220424255731}
Losses {'ner': 518.1541330859318}
Losses {'ner': 556.3773968273063}
Losses {'ner': 449.99130807122873}
Losses {'ner': 512.0942847763977}
Losses {'ner': 565.4834403758125}


----------------------------------------------------------------------------------------------------Model 5
- Name: "daphne_entities_3"
- TRAIN_DATA size: <=200
- n_iter: 50
- base_model: NONE
- drop: 0.1

Observations: 
- started better and keeps improving by the 5th iteration
- bouncing around 400, but made it to 340 at some point


Logs:


----------------------------------------------------------------------------------------------------Model 5
- Name: "daphne_entities_3"
- TRAIN_DATA size: <=200
- n_iter: 50
- base_model: NONE
- drop: 0.1

Observations: 
- started better and keeps improving by the 5th iteration
- bouncing around 300, but made it to 260 at some point
- keeps improving... made it to 149. Bouncing around 200
- made it under the 100 losses barrier

Logs:
Created blank 'en' model
Losses {'ner': 2054.5344399044343}
Losses {'ner': 956.8977561557361}
Losses {'ner': 660.5266328532073}
Losses {'ner': 714.4165436954358}
Losses {'ner': 472.3067312033699}
Losses {'ner': 450.60236490562505}
Losses {'ner': 465.0195408390295}
Losses {'ner': 532.5475329125446}
Losses {'ner': 353.2661778201134}
Losses {'ner': 448.07613938553465}
Losses {'ner': 260.4024110189594}
Losses {'ner': 283.3100723038535}
Losses {'ner': 397.8478179355974}
Losses {'ner': 287.96617294973015}
Losses {'ner': 414.6126144515398}
Losses {'ner': 276.59801616730834}
Losses {'ner': 252.597758073547}
Losses {'ner': 306.85033465835386}
Losses {'ner': 362.77294995294767}
Losses {'ner': 268.6340987738645}
Losses {'ner': 354.15857066023705}
Losses {'ner': 478.5579266991755}
Losses {'ner': 335.8227816567651}
Losses {'ner': 160.5435130482212}
Losses {'ner': 318.58275966147613}
Losses {'ner': 272.3196552619842}
Losses {'ner': 286.98149577997464}
Losses {'ner': 410.56679936650187}
Losses {'ner': 220.83293031755062}
Losses {'ner': 198.29678215562208}
Losses {'ner': 191.2764843631717}
Losses {'ner': 311.1782284336754}
Losses {'ner': 233.78231778869704}
Losses {'ner': 149.85211276646547}
Losses {'ner': 251.62896067976365}
Losses {'ner': 233.92479559542832}
Losses {'ner': 203.99045918271997}
Losses {'ner': 190.8944364929864}
Losses {'ner': 158.56184795935386}
Losses {'ner': 187.45019079511184}
Losses {'ner': 139.05466017043898}
Losses {'ner': 164.72784707630956}
Losses {'ner': 158.7637137696733}
Losses {'ner': 235.4191453649659}
Losses {'ner': 266.08905776944897}
Losses {'ner': 104.51588206838422}
Losses {'ner': 141.82606010394196}
Losses {'ner': 113.05145771676298}
Losses {'ner': 97.27684681756398}
Losses {'ner': 156.40202573379742}

----------------------------------------------------------------------------------------------------Model 6
- Name: "daphne_entities_4" (DRIVE)
- TRAIN_DATA size: <=200
- n_iter: 50
- base_model: NONE
- drop: 0.35

Observations: 
- bouncing around 500


Logs:
Losses {'ner': 2213.281689052633}
Losses {'ner': 1177.7160113540335}
Losses {'ner': 1175.4917748543664}
Losses {'ner': 1096.7655487974444}
Losses {'ner': 947.6613444922688}
Losses {'ner': 737.2701461280626}
Losses {'ner': 689.7411357275721}
Losses {'ner': 878.0137213859631}
Losses {'ner': 809.6213726866256}
Losses {'ner': 567.1616730947957}
Losses {'ner': 550.2780256546306}
Losses {'ner': 711.3066347551853}
Losses {'ner': 757.8729462093877}
Losses {'ner': 651.4865219691875}
Losses {'ner': 745.19210745882}
Losses {'ner': 569.2938783031638}
Losses {'ner': 583.2459491750981}
Losses {'ner': 574.188810417658}
Losses {'ner': 512.0607576598262}
Losses {'ner': 449.577065806898}
Losses {'ner': 484.7851436920683}
Losses {'ner': 453.90877675971285}
Losses {'ner': 627.6702932761472}
Losses {'ner': 456.965006207423}
Losses {'ner': 458.38373778113134}
Losses {'ner': 576.6638358308452}
Losses {'ner': 730.519119978278}
Losses {'ner': 582.8878734157382}
Losses {'ner': 425.72264916455595}
Losses {'ner': 508.4362204466592}
Losses {'ner': 360.36363847965396}
Losses {'ner': 565.0112488016472}
Losses {'ner': 446.3069266331859}
Losses {'ner': 421.6016942072162}
Losses {'ner': 479.1468974229226}
Losses {'ner': 351.4635238123119}
Losses {'ner': 371.44046752328416}
Losses {'ner': 550.8706873493907}
Losses {'ner': 457.7313567257131}
Losses {'ner': 411.2572890648238}
Losses {'ner': 539.3298155891544}
Losses {'ner': 593.0404296415493}
Losses {'ner': 415.99321581484975}
Losses {'ner': 330.7214788835632}
Losses {'ner': 423.295342787525}
Losses {'ner': 405.5421300587681}
Losses {'ner': 459.95925341461583}
Losses {'ner': 299.31849723692716}
Losses {'ner': 297.40464639710274}
Losses {'ner': 303.18246195855227}
----------------------------------------------------------------------------------------------------Model 7
- Name: "daphne_entities_5"
- TRAIN_DATA size: <=500
- n_iter: 100
- base_model: NONE
- drop: 0.1

Observations: 
- Making strange jumps
- decreased slower at first

Logs:
Created blank 'en' model
Losses {'ner': 3449.8314453990906} 
Losses {'ner': 1439.6053198740362} 
Losses {'ner': 1223.3000404487425} 
Losses {'ner': 1142.3521626912343} 
Losses {'ner': 957.4128240228113}  
Losses {'ner': 576.1083656001587}  
Losses {'ner': 496.978063564674}   
Losses {'ner': 567.1401856894801}  
Losses {'ner': 992.5627112295954}  
Losses {'ner': 525.1740350552673}  
Losses {'ner': 526.3478034824539}  
Losses {'ner': 336.53029384808036} 
Losses {'ner': 408.0256704492788}  
Losses {'ner': 326.62009662726075} 
Losses {'ner': 618.5587829956843}  
Losses {'ner': 385.63562369626067} 
Losses {'ner': 319.5286410584032}  
Losses {'ner': 356.57886173877}    
Losses {'ner': 390.63461575731776} 
Losses {'ner': 449.9072628592511}  
Losses {'ner': 328.2962949395468}  
Losses {'ner': 477.2611389554331}  
Losses {'ner': 267.37003014343145} 
Losses {'ner': 476.73009439123933} 
Losses {'ner': 736.8110663015415}  
Losses {'ner': 494.765534846486}   
Losses {'ner': 651.0315758735858}  
Losses {'ner': 391.7183770141857}  
Losses {'ner': 394.92839264937476} 
Losses {'ner': 478.950582624176}   
Losses {'ner': 517.9077815719805}  
Losses {'ner': 438.4859895725656}  
Losses {'ner': 482.0045908220775}  
Losses {'ner': 465.29741741391285} 
Losses {'ner': 476.46490187490633} 
Losses {'ner': 417.8805727614391}  
Losses {'ner': 368.31439655578845} 
Losses {'ner': 519.7190490360958}  
Losses {'ner': 309.9204479828466}  
Losses {'ner': 227.12681499481144} 
Losses {'ner': 309.5833650417322}  
Losses {'ner': 280.57741396905567} 
Losses {'ner': 463.57721447304084} 
Losses {'ner': 161.48333767830727} 
Losses {'ner': 383.34273813451523} 
Losses {'ner': 265.0286768004418}  
Losses {'ner': 189.03586370612683} 
Losses {'ner': 357.8533709865824}  
Losses {'ner': 447.1699237839431}  
Losses {'ner': 505.7321471003408}  
Losses {'ner': 207.13632826344815} 
Losses {'ner': 249.1578680638688}  
Losses {'ner': 198.46174082683895} 
Losses {'ner': 153.013537320685}
Losses {'ner': 171.19543448594462} 
Losses {'ner': 185.81480990761406} 
Losses {'ner': 149.8780673047032}  
Losses {'ner': 240.16287079554306} 
Losses {'ner': 203.97152560027533} 
Losses {'ner': 276.6808514725919}  
Losses {'ner': 299.04782793296386} 
Losses {'ner': 235.77016863791843} 
Losses {'ner': 216.75096823902194} 
Losses {'ner': 446.15479790178983} 
Losses {'ner': 342.500747643191}   
Losses {'ner': 323.0153102489373}  
Losses {'ner': 230.86611401233563} 
Losses {'ner': 218.42522776629988}
Losses {'ner': 226.17856167356896}
Losses {'ner': 391.9937626798035}
Losses {'ner': 230.17950565446066}
Losses {'ner': 189.7841127312147}
Losses {'ner': 196.8634522438097}
Losses {'ner': 387.18506711461987}
Losses {'ner': 275.63977459662493}
Losses {'ner': 213.75648690093837}
Losses {'ner': 189.08625217395075}
Losses {'ner': 262.71986667349415}
Losses {'ner': 231.5010251565638}
Losses {'ner': 218.8430121389926}
Losses {'ner': 178.49599818292705}
Losses {'ner': 96.16991805524421}
Losses {'ner': 319.48175471399384}
Losses {'ner': 330.4089480447019}
Losses {'ner': 210.28250222436264}
Losses {'ner': 146.52030794735418}
Losses {'ner': 269.26287692926905}

----------------------------------------------------------------------------------------------------Model 8
- Name: "daphne_entities_6" (DRIVE)
- TRAIN_DATA size: <=200
- n_iter: 100
- base_model: NONE
- drop: 0.15

Observations: 
- Started lower

Logs:
Created blank 'en' model
Losses {'ner': 1913.1187070098995}
Losses {'ner': 1039.0514437212755}
Losses {'ner': 1028.9520007291962}
Losses {'ner': 597.894089590435}
Losses {'ner': 742.8797263388489}
Losses {'ner': 545.3267472776702}
Losses {'ner': 648.251946582123}
Losses {'ner': 552.4286652384798}
Losses {'ner': 360.56812387797123}
Losses {'ner': 283.3369703983617}
Losses {'ner': 301.6329359794361}
Losses {'ner': 518.7081507708056}
Losses {'ner': 266.40182485266104}
Losses {'ner': 397.44081482450173}
Losses {'ner': 228.45873716811366}
Losses {'ner': 296.0051442309508}
Losses {'ner': 298.05853104086407}
Losses {'ner': 263.67721609054325}
Losses {'ner': 196.71848943285113}
Losses {'ner': 141.3713249127918}
Losses {'ner': 199.98370414965268}
Losses {'ner': 167.08047733159452}
Losses {'ner': 271.1477723350027}
Losses {'ner': 424.39628347201494}
Losses {'ner': 233.03496382078924}
Losses {'ner': 203.41285916723353}
Losses {'ner': 303.6192068397404}
Losses {'ner': 287.64561972142025}
Losses {'ner': 191.98977213765897}
Losses {'ner': 207.35365974571295}
Losses {'ner': 288.04825807584604}
Losses {'ner': 297.51318445384453}
Losses {'ner': 239.29689313097796}
Losses {'ner': 360.2896060389886}
Losses {'ner': 263.8617329414118}
Losses {'ner': 289.1836715665394}
Losses {'ner': 292.4192647818623}
Losses {'ner': 352.1118393277898}
Losses {'ner': 328.503545380093}
Losses {'ner': 237.99935597789}
Losses {'ner': 331.31921413435947}
Losses {'ner': 178.92936791883804}
Losses {'ner': 217.54673949627684}
Losses {'ner': 214.2053847578535}
Losses {'ner': 133.64738955204322}
Losses {'ner': 157.82746235342222}
Losses {'ner': 233.261801360498}
Losses {'ner': 252.5593012316551}
Losses {'ner': 86.57719283573314}
Losses {'ner': 139.9193852617178}
Losses {'ner': 113.82965008306384}
Losses {'ner': 111.56824974837309}
Losses {'ner': 209.69173995029985}
Losses {'ner': 205.71924362953214}
Losses {'ner': 285.83343323773073}
Losses {'ner': 136.8566771471868}
Losses {'ner': 168.2016275041048}
Losses {'ner': 186.05222633511934}
Losses {'ner': 188.18272803540668}
Losses {'ner': 219.2958068263634}
Losses {'ner': 185.4836827545039}
Losses {'ner': 188.82771136839838}
Losses {'ner': 294.5274291303772}
Losses {'ner': 247.1575701067157}
Losses {'ner': 118.15531989195537}
Losses {'ner': 117.06165262124365}
Losses {'ner': 169.7858334743312}
Losses {'ner': 233.48487585012552}
Losses {'ner': 314.4611408396646}
Losses {'ner': 110.71327708892444}
Losses {'ner': 91.52613282699855}
Losses {'ner': 146.85871141121098}
Losses {'ner': 206.61402182675505}
Losses {'ner': 147.32019714524864}
Losses {'ner': 135.93187732187994}
Losses {'ner': 207.3998216048978}
Losses {'ner': 129.61092831992445}
Losses {'ner': 302.39694781101304}
Losses {'ner': 187.71114953149817}
Losses {'ner': 129.1470799021074}
Losses {'ner': 117.2444869010763}
Losses {'ner': 267.6314487474112}
Losses {'ner': 209.03136407899285}
Losses {'ner': 130.2381379224191}
Losses {'ner': 129.9836520405734}
Losses {'ner': 215.66752594466618}
Losses {'ner': 122.35032974360855}
Losses {'ner': 178.6625678646853}
Losses {'ner': 167.30338180537177}
Losses {'ner': 158.20882245767342}
Losses {'ner': 156.96372784829447}
Losses {'ner': 113.7241717066754}
Losses {'ner': 138.78984822374534}
Losses {'ner': 190.82193412888992}
Losses {'ner': 149.2893068190632}
Losses {'ner': 136.20896438614264}
Losses {'ner': 208.87743186218202}
Losses {'ner': 74.9192124601914}
Losses {'ner': 82.06017873061765}
Losses {'ner': 158.5668107341446}

----------------------------------------------------------------------------------------------------Model 9
- Name: "daphne_entities_7"
- TRAIN_DATA size: <=500
- n_iter: 50
- base_model: None
- drop: 0.15
- compounding(1.0, 20.0, 1.01)

Observations: 
- Many values bellow 100 losses (may do better than previous experiments)
- Worked way better than previous models. Apparentlty changing the batch size was a good way to go.

Logs:
Created blank 'en' model
Losses {'ner': 2681.08407204496}
Losses {'ner': 281.22051301537283}
Losses {'ner': 237.47801805985154}
Losses {'ner': 64.1701811880434}
Losses {'ner': 99.18399256903531}
Losses {'ner': 129.19799708378804}
Losses {'ner': 165.07780073419664}
Losses {'ner': 103.84054471599256}
Losses {'ner': 173.7821476639761}
Losses {'ner': 93.55485611681804}
Losses {'ner': 119.55119675265375}
Losses {'ner': 178.88498659570044}
Losses {'ner': 246.0324880439956}
Losses {'ner': 46.240846830180054}
Losses {'ner': 105.8170248063347}
Losses {'ner': 66.46486627835213}
Losses {'ner': 79.56496718099571}
Losses {'ner': 107.57994276989574}
Losses {'ner': 128.3595013483751}
Losses {'ner': 74.56817598498843}
Losses {'ner': 102.28845472347348}
Losses {'ner': 109.69358231618527}
Losses {'ner': 36.6684506829047}
Losses {'ner': 53.52997271589325}
Losses {'ner': 143.47032399164004}
Losses {'ner': 80.1023665984627}
Losses {'ner': 46.19477698602909}
Losses {'ner': 85.83890229071834}
Losses {'ner': 95.38741255330774}
Losses {'ner': 125.95137384211657}
Losses {'ner': 132.01208717000335}
Losses {'ner': 108.49900407940223}
Losses {'ner': 46.369338541922446}
Losses {'ner': 16.8708368643576}
Losses {'ner': 34.81801773145697}
Losses {'ner': 78.2624103369489}
Losses {'ner': 37.221733515597286}
Losses {'ner': 54.62656470961585}
Losses {'ner': 113.98512354375615}
Losses {'ner': 157.59608721894435}
Losses {'ner': 36.45097117364722}
Losses {'ner': 93.49529991232464}
Losses {'ner': 103.84361246149362}
Losses {'ner': 38.07030115510527}
Losses {'ner': 60.3517619949948}
Losses {'ner': 79.84108543683824}
Losses {'ner': 88.92015623195216}
Losses {'ner': 73.19035835107215}
Losses {'ner': 107.13562184138299}
Losses {'ner': 109.7847997866482}

----------------------------------------------------------------------------------------------------Model 10
- Name: "daphne_entities_8" (DRIVE)
- TRAIN_DATA size: <=1000
- n_iter: 50
- base_model: NONE
- drop: 0.08

Observations: 
- not that good of a model

Logs:
Created blank 'en' model
Losses {'ner': 4356.412581768616}
Losses {'ner': 1753.3545703309026}
Losses {'ner': 1609.8254599348645}
Losses {'ner': 1344.1485020429384}
Losses {'ner': 1100.9086791083953}
Losses {'ner': 1304.317877513708}
Losses {'ner': 887.4148576908376}
Losses {'ner': 743.160142851377}
Losses {'ner': 1000.7717083664413}
Losses {'ner': 1189.404191947989}
Losses {'ner': 950.6100886801069}
Losses {'ner': 695.2522934242003}
Losses {'ner': 556.0260125389744}
Losses {'ner': 617.1379050114372}
Losses {'ner': 448.3281607981269}
Losses {'ner': 747.9626267159194}
Losses {'ner': 873.1583414026869}
Losses {'ner': 519.175743069083}
Losses {'ner': 800.1297580911081}
Losses {'ner': 763.3393679204912}
Losses {'ner': 417.7846195600993}
Losses {'ner': 768.5588280478158}
Losses {'ner': 474.6658427946547}
Losses {'ner': 595.8363649654383}
Losses {'ner': 676.1548417648086}
Losses {'ner': 482.4115708806936}
Losses {'ner': 783.5691051117274}
Losses {'ner': 841.8748448128172}
Losses {'ner': 874.9336530197697}
Losses {'ner': 844.5545721831324}
Losses {'ner': 555.2540055840116}
Losses {'ner': 614.6905440630426}
Losses {'ner': 837.0697219555918}
Losses {'ner': 681.876720516432}
Losses {'ner': 480.46874905479285}
Losses {'ner': 370.572169482445}
Losses {'ner': 826.1829008157094}
Losses {'ner': 477.24135640900676}
Losses {'ner': 486.9587359596536}
Losses {'ner': 566.2108804535136}
Losses {'ner': 599.8708375076438}
Losses {'ner': 515.1296025280158}
Losses {'ner': 393.62276643100665}
Losses {'ner': 311.7414189536286}
Losses {'ner': 432.3745021063588}
Losses {'ner': 382.11544922266967}
Losses {'ner': 386.98407536315864}
Losses {'ner': 439.78195418177506}
Losses {'ner': 540.9592492279403}
Losses {'ner': 394.82620718917445}

----------------------------------------------------------------------------------------------------Model 11
- Name: "daphne_entities_9" (CONTAINS AGENT PARAM)
- TRAIN_DATA size: <=500
- n_iter: 50
- base_model: None
- drop: 0.15
- compounding(1.0, 20.0, 1.01)

Observations: 

Logs:
Created blank 'en' model
Losses {'ner': 2232.6090395949186}
Losses {'ner': 299.1115949614703}
Losses {'ner': 194.25754215723623}
Losses {'ner': 123.1463494011428}
Losses {'ner': 94.1703165732963}
Losses {'ner': 115.02450595580454}
Losses {'ner': 62.528562981315616}
Losses {'ner': 94.72995475588795}
Losses {'ner': 63.6173241704372}
Losses {'ner': 130.61058421761453}
Losses {'ner': 90.31408657908837}
Losses {'ner': 59.679266568433576}
Losses {'ner': 242.18464072534417}
Losses {'ner': 128.88463084280943}
Losses {'ner': 77.1033428749047}
Losses {'ner': 50.15165682901329}
Losses {'ner': 66.91000274054329}
Losses {'ner': 56.80110382573168}
Losses {'ner': 27.569759669422677}
Losses {'ner': 171.44648388281684}
Losses {'ner': 103.06407463481204}
Losses {'ner': 71.4550687310555}
Losses {'ner': 55.20708296387365}
Losses {'ner': 15.034884299825663}
Losses {'ner': 26.17129448885714}
Losses {'ner': 21.080158412879914}
Losses {'ner': 89.8702515632862}
Losses {'ner': 173.581113727657}
Losses {'ner': 45.489998551945284}
Losses {'ner': 39.466604839709994}
Losses {'ner': 115.05280062302037}
Losses {'ner': 60.54925874991516}
Losses {'ner': 22.690770977215184}
Losses {'ner': 12.842272317827303}
Losses {'ner': 156.06217800076732}
Losses {'ner': 122.84745336657024}
Losses {'ner': 107.94149569827862}
Losses {'ner': 95.67204088120502}
Losses {'ner': 56.55499511662393}
Losses {'ner': 68.46536174454499}
Losses {'ner': 49.37297960528524}
Losses {'ner': 125.72831138654297}
Losses {'ner': 158.6559527629049}
Losses {'ner': 41.60343397455298}
Losses {'ner': 107.85658720163904}
Losses {'ner': 144.65411875825612}
Losses {'ner': 59.88379972967454}
Losses {'ner': 53.85079319414626}
Losses {'ner': 56.667415455668156}
Losses {'ner': 43.25541086819246}

----------------------------------------------------------------------------------------------------Model 12
- Name: "daphne_entities_10"
- TRAIN_DATA size: <=500
- n_iter: 50
- base_model: None
- drop: 0.08
- compounding(1.0, 20.0, 1.01)

Observations: 
- Added design ids with lower case d
Logs:
Created blank 'en' model
Losses {'ner': 1976.9088666196917}
Losses {'ner': 149.7707267165616}
Losses {'ner': 81.17085431810382}
Losses {'ner': 145.53593073998508}
Losses {'ner': 114.21396307928578}
Losses {'ner': 80.37160539518091}
Losses {'ner': 98.17923599981124}
Losses {'ner': 68.12335234041375}
Losses {'ner': 95.18181114323505}
Losses {'ner': 41.31802224078324}
Losses {'ner': 218.45205884454435}
Losses {'ner': 146.88906575905708}
Losses {'ner': 212.36282585540454}
Losses {'ner': 163.14899644237053}
Losses {'ner': 79.53146991962089}
Losses {'ner': 50.560302062056124}
Losses {'ner': 109.83069869493356}
Losses {'ner': 20.82505264139287}
Losses {'ner': 137.1122173341244}
Losses {'ner': 33.18704616356511}
Losses {'ner': 78.15419552597123}
Losses {'ner': 16.04154319377411}
Losses {'ner': 67.25053764470196}
Losses {'ner': 90.59549927410205}
Losses {'ner': 115.58563338275947}
Losses {'ner': 37.15478269416553}
Losses {'ner': 51.2469898042379}
Losses {'ner': 44.569056554295365}
Losses {'ner': 36.31704184551225}
Losses {'ner': 22.944811704453343}
Losses {'ner': 13.417256825756583}
Losses {'ner': 83.02078960457492}
Losses {'ner': 34.55937518991354}
Losses {'ner': 138.9296808594498}
Losses {'ner': 41.83830368385573}
Losses {'ner': 56.10362712799656}
Losses {'ner': 29.732412185981087}
Losses {'ner': 116.26259076290013}
Losses {'ner': 84.91507720985786}
Losses {'ner': 12.200101485332798}
Losses {'ner': 2.0437406908511666}
Losses {'ner': 125.96461105059619}
Losses {'ner': 83.10135084394047}
Losses {'ner': 34.40092122961848}
Losses {'ner': 50.92230739725534}
Losses {'ner': 35.85751559567828}
Losses {'ner': 40.246147096926244}
Losses {'ner': 135.30439976513284}
Losses {'ner': 92.4254788599116}
Losses {'ner': 92.80136534879689}




----------------------------------------------------------------------------------------------------Model 13
- Name: "daphne_entities_11"
- TRAIN_DATA size: <=500
- n_iter: 50
- base_model: None
- drop: 0.2
- compounding(1.0, 20.0, 1.01)

Observations: 
- 

Logs:

Created blank 'en' model
Losses {'ner': 2838.330111888019}
Losses {'ner': 232.85558407410252}
Losses {'ner': 222.4782345683117}
Losses {'ner': 230.6564329549776}
Losses {'ner': 224.88007303519714}
Losses {'ner': 212.46873746401977}
Losses {'ner': 130.67493387288147}
Losses {'ner': 161.62433856309326}
Losses {'ner': 57.034435870641786}
Losses {'ner': 64.69425832852559}
Losses {'ner': 179.90878429357232}
Losses {'ner': 121.87605069294689}
Losses {'ner': 150.28110262851555}
Losses {'ner': 117.7804334509931}
Losses {'ner': 64.03940237051646}
Losses {'ner': 89.00747056198004}
Losses {'ner': 78.0420638253737}
Losses {'ner': 161.09569935916403}
Losses {'ner': 107.87173115143224}
Losses {'ner': 93.81625064425535}
Losses {'ner': 56.817724493425224}
Losses {'ner': 20.693766118992848}
Losses {'ner': 115.19469093145399}
Losses {'ner': 237.8599463054919}
Losses {'ner': 150.70977889787068}
Losses {'ner': 198.0596338885219}
Losses {'ner': 79.78908077780206}
Losses {'ner': 115.94096740037318}
Losses {'ner': 69.56102484822529}
Losses {'ner': 57.90937034145409}
Losses {'ner': 75.67079025676048}
Losses {'ner': 117.09460308249045}
Losses {'ner': 96.05652268735994}
Losses {'ner': 101.82985597832227}
Losses {'ner': 115.0044832445825}
Losses {'ner': 158.46511683138957}
Losses {'ner': 138.39087756479185}
Losses {'ner': 190.37832106512124}
Losses {'ner': 142.86990913844585}
Losses {'ner': 121.03301144573568}
Losses {'ner': 67.11490911866859}
Losses {'ner': 147.12650026211875}
Losses {'ner': 75.42332116421113}
Losses {'ner': 123.76476745998235}
Losses {'ner': 173.8777240117138}
Losses {'ner': 97.98307382444759}
Losses {'ner': 51.54487109349253}
Losses {'ner': 70.27768813140946}
Losses {'ner': 187.82284390621797}
Losses {'ner': 105.33920185543786}


----------------------------------------------------------------------------------------------------Model 14
- Name: "daphne_entities_12"
- TRAIN_DATA size: <=500
- n_iter: 50
- base_model: None
- drop: 0.2
- compounding(1.0, 20.0, 1.01)

Observations: 
- Every file will be shuffled before beeing cut to the threshold

Logs:
Created blank 'en' model
Losses {'ner': 3022.143869583192}
Losses {'ner': 317.8798165109055}
Losses {'ner': 149.42946221904788}
Losses {'ner': 195.40701216356769}
Losses {'ner': 216.69621343049977}
Losses {'ner': 170.3129496840264}
Losses {'ner': 102.2241257400462}
Losses {'ner': 278.94783986230306}
Losses {'ner': 142.9512309345215}
Losses {'ner': 208.25257072620929}
Losses {'ner': 237.5190265004668}
Losses {'ner': 159.81755343758385}
Losses {'ner': 254.2707164686665}
Losses {'ner': 198.29061048761056}
Losses {'ner': 73.35539401878732}
Losses {'ner': 154.54314112036332}
Losses {'ner': 118.98169222047227}
Losses {'ner': 112.3571455309112}
Losses {'ner': 132.60869480512727}
Losses {'ner': 54.36318727244306}
Losses {'ner': 50.163872862509436}
Losses {'ner': 159.02129278491725}
Losses {'ner': 106.23736751671346}
Losses {'ner': 91.68372101384386}
Losses {'ner': 151.42880618650636}
Losses {'ner': 140.07751944893508}
Losses {'ner': 151.9836076487193}
Losses {'ner': 140.94471651488394}
Losses {'ner': 235.61834692063692}
Losses {'ner': 168.88081216155285}
Losses {'ner': 78.91215506548345}
Losses {'ner': 150.64172209156195}
Losses {'ner': 124.84446397034979}
Losses {'ner': 159.69600958073525}
Losses {'ner': 132.20704331396354}
Losses {'ner': 78.99060776187741}
Losses {'ner': 100.47166234541179}
Losses {'ner': 86.67934229552549}
Losses {'ner': 160.7317732729829}
Losses {'ner': 160.28628778323923}
Losses {'ner': 185.73646931303162}
Losses {'ner': 186.11257327441518}
Losses {'ner': 146.9169765169062}
Losses {'ner': 144.91393271384894}
Losses {'ner': 108.1717042037763}
Losses {'ner': 103.02355392660508}
Losses {'ner': 89.77526522078111}
Losses {'ner': 72.1736728644036}
Losses {'ner': 48.790291341258936}
Losses {'ner': 60.35584769824468}

----------------------------------------------------------------------------------------------------Model 15
- Name: "daphne_entities_13"
- TRAIN_DATA size: <=500
- n_iter: 100
- base_model: None
- drop: 0.2
- compounding(1.0, 20.0, 1.01)

Observations: 
- Every file will be shuffled before beeing cut to the threshold

Logs: